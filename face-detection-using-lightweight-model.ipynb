{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9456975,"sourceType":"datasetVersion","datasetId":5748968}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nimport numpy as np\n\n# Define dataset path\ndataset_path = \"/kaggle/input/face-detection-by-kazmir/Face detection\"\n\n# Load dataset with data augmentation\nbatch_size = 16\nimg_size = (224, 224)\n\n# Load the dataset before augmentation to retrieve class names\ntrain_ds, val_ds = image_dataset_from_directory(\n    dataset_path,\n    validation_split=0.2,\n    subset=\"both\",\n    seed=123,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\n# Check the number of classes by inspecting the class names\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Classes found: {class_names}\")\n\n# Now apply data augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip('horizontal'),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.2),\n])\n\n# Apply the augmentation to the dataset\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n\n# Use the MobileNetV2 pre-trained model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the base model to use it as a feature extractor\nbase_model.trainable = False\n\n# Add classification layers on top with dropout to prevent overfitting\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Global average pooling layer\nx = Dense(128, activation='relu')(x)  # Fully connected layer\nx = Dropout(0.5)(x)  # Add dropout layer with 50% dropout rate\npredictions = Dense(num_classes, activation='softmax')(x)  # Number of classes\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nepochs = 20  # Increased the number of epochs\nhistory = model.fit(train_ds, epochs=epochs)\n\n# Save the model\nmodel.save('/kaggle/working/face_detection_model_with_augmentation.h5')\n\n# Save the TensorFlow Lite model\nwith open('/kaggle/working/face_detection_model_with_augmentation.tflite', 'wb') as f:\n    f.write(tflite_model)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T13:43:38.840556Z","iopub.execute_input":"2024-09-26T13:43:38.841642Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found 25 files belonging to 5 classes.\nUsing 20 files for training.\nUsing 5 files for validation.\nClasses found: ['Kazmir', 'Mudassir', 'ashik', 'riyan', 'sayed']\nEpoch 1/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 104ms/step - accuracy: 0.2167 - loss: 2.5849\nEpoch 2/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.3375 - loss: 1.9081\nEpoch 3/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.3042 - loss: 1.2626\nEpoch 4/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 1.3317\nEpoch 5/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.4250 - loss: 1.3687\nEpoch 6/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.5750 - loss: 1.2017\nEpoch 7/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.5750 - loss: 1.2660\nEpoch 8/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.7500 - loss: 0.5391\nEpoch 9/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.7292 - loss: 0.6287\nEpoch 10/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.5542 - loss: 0.7997\nEpoch 11/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7292 - loss: 0.6428\nEpoch 12/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8917 - loss: 0.3205\nEpoch 13/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.6292 - loss: 0.7043\nEpoch 14/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.8375 - loss: 0.4753\nEpoch 15/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.7292 - loss: 0.5996\nEpoch 16/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.9333 - loss: 0.3668\nEpoch 17/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9458 - loss: 0.2836\nEpoch 18/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.2074\nEpoch 19/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.8375 - loss: 0.3953\nEpoch 20/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.7292 - loss: 0.4749\n","output_type":"stream"}]}]}
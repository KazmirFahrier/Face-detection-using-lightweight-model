{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9456975,"sourceType":"datasetVersion","datasetId":5748968}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\n# Define dataset path\ndataset_path = \"/kaggle/input/face-detection-by-kazmir/Face detection\"\n\n# Load dataset with data augmentation\nbatch_size = 16\nimg_size = (224, 224)\n\n# Load the dataset before augmentation to retrieve class names\ntrain_ds, val_ds = image_dataset_from_directory(\n    dataset_path,\n    validation_split=0.2,\n    subset=\"both\",\n    seed=123,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\n# Check the number of classes by inspecting the class names\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Classes found: {class_names}\")\n\n# Now apply data augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip('horizontal'),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.2),\n    layers.RandomContrast(0.2),\n    layers.RandomBrightness(0.2),\n    layers.GaussianNoise(0.1),  # Adding Gaussian noise\n])\n\n\n# Apply the augmentation to the dataset\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n\n# Use the MobileNetV2 pre-trained model\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the base model to use it as a feature extractor\nbase_model.trainable = False\n\n# Add classification layers on top with dropout to prevent overfitting\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Global average pooling layer\nx = Dense(128, activation='relu')(x)  # Fully connected layer\nx = Dropout(0.5)(x)  # Add dropout layer with 50% dropout rate\npredictions = Dense(num_classes, activation='softmax')(x)  # Number of classes\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Define the EarlyStopping callback\nearly_stopping = EarlyStopping(\n    monitor='val_loss',  \n    patience=5,          # Number of epochs with no improvement after which training will be stopped\n    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored metric\n)\n\n\n# Train the model\nepochs = 20  # Increased the number of epochs\n\n# Fit the model with the early stopping callback\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,  \n    epochs=20,               \n    callbacks=[early_stopping]\n)\n\n# Save the model\nmodel.save('/kaggle/working/face_detection_model_with_augmentation.h5')\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-26T16:11:26.824165Z","iopub.execute_input":"2024-09-26T16:11:26.825043Z","iopub.status.idle":"2024-09-26T16:11:55.252429Z","shell.execute_reply.started":"2024-09-26T16:11:26.824994Z","shell.execute_reply":"2024-09-26T16:11:55.251226Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 25 files belonging to 5 classes.\nUsing 20 files for training.\nUsing 5 files for validation.\nClasses found: ['Kazmir', 'Mudassir', 'ashik', 'riyan', 'sayed']\nEpoch 1/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.3167 - loss: 1.7688 - val_accuracy: 0.4000 - val_loss: 1.5452\nEpoch 2/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step - accuracy: 0.3917 - loss: 1.4493 - val_accuracy: 0.4000 - val_loss: 1.2781\nEpoch 3/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step - accuracy: 0.6083 - loss: 0.9477 - val_accuracy: 0.6000 - val_loss: 1.1494\nEpoch 4/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step - accuracy: 0.5875 - loss: 1.0833 - val_accuracy: 0.6000 - val_loss: 1.0132\nEpoch 5/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step - accuracy: 0.5333 - loss: 1.0822 - val_accuracy: 0.6000 - val_loss: 0.9493\nEpoch 6/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 0.7167 - loss: 0.9696 - val_accuracy: 0.6000 - val_loss: 0.9846\nEpoch 7/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.5000 - loss: 0.9754 - val_accuracy: 0.2000 - val_loss: 1.0588\nEpoch 8/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - accuracy: 0.8583 - loss: 0.7244 - val_accuracy: 0.4000 - val_loss: 1.0035\nEpoch 9/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step - accuracy: 0.5875 - loss: 0.9439 - val_accuracy: 0.4000 - val_loss: 0.8291\nEpoch 10/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - accuracy: 0.7292 - loss: 1.0645 - val_accuracy: 0.8000 - val_loss: 0.5680\nEpoch 11/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 0.6292 - loss: 0.7740 - val_accuracy: 0.8000 - val_loss: 0.4331\nEpoch 12/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step - accuracy: 0.8583 - loss: 0.5628 - val_accuracy: 1.0000 - val_loss: 0.4178\nEpoch 13/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 208ms/step - accuracy: 0.6417 - loss: 0.7689 - val_accuracy: 1.0000 - val_loss: 0.4463\nEpoch 14/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 0.6417 - loss: 1.1184 - val_accuracy: 1.0000 - val_loss: 0.3805\nEpoch 15/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - accuracy: 0.8042 - loss: 0.7009 - val_accuracy: 1.0000 - val_loss: 0.2964\nEpoch 16/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step - accuracy: 0.8583 - loss: 0.3307 - val_accuracy: 1.0000 - val_loss: 0.2596\nEpoch 17/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - accuracy: 0.8250 - loss: 0.4624 - val_accuracy: 1.0000 - val_loss: 0.2723\nEpoch 18/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step - accuracy: 0.8250 - loss: 0.3688 - val_accuracy: 1.0000 - val_loss: 0.2784\nEpoch 19/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 0.6958 - loss: 0.7042 - val_accuracy: 1.0000 - val_loss: 0.2543\nEpoch 20/20\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step - accuracy: 0.8583 - loss: 0.3745 - val_accuracy: 1.0000 - val_loss: 0.2571\n","output_type":"stream"}]}]}